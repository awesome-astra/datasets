{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "174d7d16-6c42-4af6-a550-c6becae42b43",
   "metadata": {},
   "source": [
    "```\n",
    "Required python dependencies\n",
    "\n",
    "transformers==4.33.3\n",
    "timm==0.9.7\n",
    "cassio>=0.1.3\n",
    "datasets==2.13.1\n",
    "gradio==3.36.1\n",
    "jupyter>=1.0.0\n",
    "numpy==1.24.4\n",
    "panns_inference==0.1.1\n",
    "python-dotenv==1.0.0\n",
    "scipy>=1.10\n",
    "torch==2.0.1\n",
    "torchvision==0.15.2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a538105-8ca4-467b-84cd-dc0d0d4357c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TARGET_SIZES = [100, 500, 1000, 2000, 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6ef8b84-1b12-42ef-aeee-df7fb219de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    CenterCrop,\n",
    "    ToTensor,\n",
    "    Normalize\n",
    ")\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ad5c931-d697-464d-925e-f9f413bdf35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Compose([\n",
    "    Resize(256),\n",
    "    CenterCrop(224),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b837aec0-cd19-47a2-a610-8d09c6aaad16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "DATA_DIRECTORY = \"data\"\n",
    "\n",
    "datasets = {\n",
    "    \"CIFAR10\": torchvision.datasets.CIFAR10(\n",
    "        DATA_DIRECTORY,\n",
    "        transform=preprocess,\n",
    "        download=True,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1c15ba9-85e8-4f34-9c43-ad2ae771f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4d07ba7-ec8e-4c12-b8d7-5e1f720b5360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total selected images: 5000\n"
     ]
    }
   ],
   "source": [
    "img_per_class = (1 + max(DATASET_TARGET_SIZES)) // len(labels_dict)\n",
    "n_classes = len(labels_dict)\n",
    "sel_img_map = {i: [] for i in range(n_classes)}\n",
    "labels_map = {i: [] for i in range(n_classes)}\n",
    "sel_img_arr_map = {i: [] for i in range(n_classes)}\n",
    "\n",
    "# Iterate through each class and select a number of images\n",
    "for (img, cl_label), img_arr in zip(datasets[\"CIFAR10\"], datasets[\"CIFAR10\"].data):\n",
    "\n",
    "    # are all classes completed?\n",
    "    if all(len(ims) >= img_per_class for ims in sel_img_map.values()):\n",
    "        break\n",
    "\n",
    "    if len(sel_img_map[cl_label]) < img_per_class:\n",
    "        # add this one\n",
    "        sel_img_map[cl_label].append(img)\n",
    "        labels_map[cl_label].append(cl_label)\n",
    "        sel_img_arr_map[cl_label].append(img_arr)\n",
    "\n",
    "selected_images = []\n",
    "labels = []\n",
    "selected_img_array = []\n",
    "for cl_label in range(n_classes):\n",
    "    selected_images += sel_img_map[cl_label]\n",
    "    labels += labels_map[cl_label]\n",
    "    selected_img_array += sel_img_arr_map[cl_label]\n",
    "print(\"Total selected images:\", len(selected_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3801f0ed-404b-48fa-9d38-d6e7527233eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_AVAILABLE = torch.cuda.device_count() > 0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torchvision.models.squeezenet1_1(\n",
    "    weights=torchvision.models.SqueezeNet1_1_Weights.IMAGENET1K_V1\n",
    ").to(device).eval()\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    print(\"Loaded the image embedding model on the GPU.\")\n",
    "else:\n",
    "    print(\"Loaded the image embedding model on the CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b028947f-9ce8-4601-90d1-657e3a3e4a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_metadata(label_indices, class_list):\n",
    "    \"\"\"Return list of {\"label\": <class name>}.\"\"\"\n",
    "    return [{\"label\": class_list[index]} for index in label_indices]\n",
    "\n",
    "# Creating Vector IDs\n",
    "# Each vector ID will have a prefix corresponding to CIFAR10\n",
    "def get_vector_ids(batch_number, batch_size, prefix):\n",
    "    \"\"\"Return vector ids.\"\"\"\n",
    "    start_index = batch_number\n",
    "    end_index = start_index + batch_size\n",
    "    ids = np.arange(start_index, end_index)\n",
    "\n",
    "    # create id based on prefix\n",
    "    # eg. if id == 5, prefix == \"CIFAR10\", then create \"CIFAR10.5\" as vector id.\n",
    "    ids_with_prefix = [f\"{prefix}.{str(x)}\" for x in ids]\n",
    "    return ids_with_prefix\n",
    "\n",
    "\n",
    "def get_vectors_from_batch(data_processed, label_indices,batch_number, dataset):\n",
    "    \"\"\"Return list of tuples like (vector_id, vector_values, vector_metadata).\"\"\"\n",
    "    num_records = len(data_processed)\n",
    "    prefix = dataset.__class__.__name__\n",
    "    with torch.no_grad():\n",
    "        # generate image embeddings with PyTorch model\n",
    "        vector_values = model(data_processed).tolist()\n",
    "    # return respective IDs/metadata for each image embedding\n",
    "    vector_metadata = get_vector_metadata(label_indices, dataset.classes)\n",
    "    vector_ids = get_vector_ids(batch_number, num_records, prefix)\n",
    "    return list(zip(vector_ids, vector_values, vector_metadata))\n",
    "\n",
    "dataset = datasets[\"CIFAR10\"]\n",
    "# Move the data to the respective device\n",
    "preprocessed_data = torch.stack(selected_images).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "253bb0da-4b4c-451b-be54-a242bb55e41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google/vit-base-patch16-224 and revision 5dca96d (https://huggingface.co/google/vit-base-patch16-224).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# further labeling\n",
    "tr = T.ToPILImage()\n",
    "lab_model=pipeline(\"image-classification\")\n",
    "\n",
    "def label_img(img_data):\n",
    "    res = lab_model(tr(img_data))\n",
    "    if res:\n",
    "        pt = res[0]['label'].split(',')[0]\n",
    "        if pt:\n",
    "            return pt\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1960d67-8dd9-4030-b6ff-9cd12da9942b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3374f1514ba4b919080b90325f45c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "SAMPLES_TO_PROCESS = len(selected_images)\n",
    "\n",
    "full_items = []\n",
    "\n",
    "for i in tqdm(range(0, SAMPLES_TO_PROCESS, BATCH_SIZE)):\n",
    "    # Find end of batch\n",
    "    i_end = min(i + BATCH_SIZE, SAMPLES_TO_PROCESS)\n",
    "    # Generate embeddings for all the images in the batch\n",
    "    # with the corresponding vector id and metadata lists\n",
    "    batch_vectors = get_vectors_from_batch(\n",
    "        preprocessed_data[i:i_end],\n",
    "        labels[i:i_end],\n",
    "        i,\n",
    "        dataset,\n",
    "    )\n",
    "\n",
    "    for deltai, (vector_id, embedding, metadata) in enumerate(batch_vectors):\n",
    "        img_label = label_img(selected_img_array[i+deltai])\n",
    "        full_items.append(\n",
    "            {\n",
    "                'id': vector_id,\n",
    "                'embedding': embedding,\n",
    "                'label': metadata['label'],\n",
    "                'metadata': {'content': img_label},\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61a3a7f5-2863-4894-b78a-ec4b645b6ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _permute(lst, n):\n",
    "    return list(np.random.permutation(lst)[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5df45489-49dd-4136-9450-d35df0335988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutations and save all at once\n",
    "all_datasets = {\n",
    "    size: _permute(full_items, size)\n",
    "    for size in DATASET_TARGET_SIZES\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffd19fd8-94b1-4f70-9fee-8c212e9ec8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def make_json(itm):\n",
    "    return itm\n",
    "\n",
    "def _make_csv_em(v):\n",
    "    return '\"'+str(v)+'\"'\n",
    "\n",
    "def _make_csv_md(md):\n",
    "    return '\"'+json.dumps(md, separators=[',', ':']).replace('\"', '\\\\\"')+'\"'\n",
    "\n",
    "def make_csv_dict(itm):\n",
    "    return {\n",
    "        'id': itm['id'],\n",
    "        'embedding': _make_csv_em(itm['embedding']),\n",
    "        'label': itm['label'],\n",
    "        'metadata': _make_csv_md(itm['metadata']),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef74cc78-ce85-451a-8711-e57f1a51d0a4",
   "metadata": {},
   "source": [
    "#### manual hacking of the CSV output for dsbulk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30cbec8b-36f6-41ab-a6e1-f58ac91dff18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"{\\\"k1\\\":\\\"v1\\\",\\\"k2\\\":\\\"v2\\\"}\"\n"
     ]
    }
   ],
   "source": [
    "md = {'k1': 'v1', 'k2': 'v2'}\n",
    "\n",
    "print(_make_csv_md(md))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ec58b6-4ba8-4a89-8e2e-4d399d27c839",
   "metadata": {},
   "source": [
    "`\"{\\\"k1\\\":\\\"v1\\\",\\\"k2\\\":\\\"v2\\\"}\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "241da6d0-f72c-44b6-9e11-4002b03ada96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"[0.1, 0.3333333333333339, 0.3]\"\n"
     ]
    }
   ],
   "source": [
    "v = [0.1, 0.3333333333333339, 0.3]\n",
    "print(_make_csv_em(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69c97bc-faf4-448e-8477-7651e8862b99",
   "metadata": {},
   "source": [
    "#### manual hacking of the JSON output to keep the numbers short for dsbulk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c16e9bb-8b33-4a9c-9b30-713eee1db2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## inspired by:\n",
    "## https://stackoverflow.com/questions/54370322/how-to-limit-the-number-of-float-digits-jsonencoder-produces\n",
    "\n",
    "orig_dict = {'a': 'letter', 'v': [2.5328171253204346, 4.954568386077881]}\n",
    "\n",
    "print(json.dumps(orig_dict, indent=4, default=json_handler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edd6e2d-b268-46e8-906d-eea6eaccd048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoundingFloat(float):\n",
    "    __repr__ = staticmethod(lambda x: format(x, '.5f'))\n",
    "\n",
    "json.encoder.c_make_encoder = None\n",
    "if hasattr(json.encoder, 'FLOAT_REPR'):\n",
    "    # Python 2\n",
    "    json.encoder.FLOAT_REPR = RoundingFloat.__repr__\n",
    "else:\n",
    "    # Python 3\n",
    "    json.encoder.float = RoundingFloat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9699d705-046e-4497-9d11-f7f5f6f7be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(orig_dict, indent=4, default=json_handler))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbc2d3f-4ab6-4d22-8fcf-f0d9a7c59df7",
   "metadata": {},
   "source": [
    "## Hackings completed, we can print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2431ad0a-8fb8-4f69-bd6c-e4d0b10f65ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ebd09327-b3d1-472e-a281-447ed60fb1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.register_dialect('dsbulk', escapechar='\\\\')\n",
    "\n",
    "for size, items in all_datasets.items():\n",
    "    fname = 'cifar10-data-%s.csv' % size\n",
    "    with open(fname, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['id', 'embedding', 'label', 'metadata']\n",
    "        csvfile.write(f\"{','.join(fieldnames)}\\n\")\n",
    "        # writer = csv.DictWriter(csvfile, dialect='dsbulk', fieldnames=fieldnames)\n",
    "    \n",
    "        # writer.writeheader()\n",
    "        for itm in items:\n",
    "            citm = make_csv_dict(itm)\n",
    "            csvfile.write(f\"{citm['id']},{citm['embedding']},{citm['label']},{citm['metadata']}\\n\")\n",
    "\n",
    "for size, items in all_datasets.items():\n",
    "    fname = 'cifar10-data-%s-no-embedding.csv' % size\n",
    "    with open(fname, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['id', 'label', 'metadata']\n",
    "        csvfile.write(f\"{','.join(fieldnames)}\\n\")\n",
    "        # writer = csv.DictWriter(csvfile, dialect='dsbulk', fieldnames=fieldnames)\n",
    "    \n",
    "        # writer.writeheader()\n",
    "        for itm in items:\n",
    "            citm = make_csv_dict(itm)\n",
    "            csvfile.write(f\"{citm['id']},{citm['label']},{citm['metadata']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1618fae-60b1-434a-b447-2c0bf8c13cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57d3c11b-66c3-403f-a760-c6666196e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "for size, items in all_datasets.items():\n",
    "    fname = 'cifar10-data-%s.json' % size\n",
    "    with open(fname, 'w') as jsonfile:\n",
    "        json.dump(items, jsonfile, indent=1)\n",
    "\n",
    "for size, items in all_datasets.items():\n",
    "    fname = 'cifar10-data-%s-no-embedding.json' % size\n",
    "    with open(fname, 'w') as jsonfile:\n",
    "        no_e_items = [\n",
    "            {\n",
    "                k: v\n",
    "                for k, v in itm.items()\n",
    "                if k != 'embedding'\n",
    "            }\n",
    "            for itm in items\n",
    "        ]\n",
    "        json.dump(no_e_items, jsonfile, indent=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
